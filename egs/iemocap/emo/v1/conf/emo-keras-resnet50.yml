---
data:
  train:
    paths:
    - "data/dump/all/train"
    segments:
  eval:
    paths:
    - "data/dump/all/eval"
    segments:
  infer:
    paths:
    - "data/dump/all/eval"
    segments:
  task:
    name: IEmoCapTask 
    suffix: .npy # file suffix
    subset: 'all' #impro, script, all
    audio:
      dry_run: false # not save feat
      # params
      clip_size: 7 # clip len in seconds
      stride: 0.5 # stride in ratio of clip_size
      sr: 16000 # sample rate
      winlen: 0.025 # window len
      winstep: 0.01 # window stride
      nfft: 512 # fft bins, default: 512
      lowfreq: 0
      highfreq: null # default: null, 200 points for 800 nfft, 400 points for 1600 nfft
      preemph: 0.97 # default: 0.97
      # extractor
      feature_extractor: tffeat # `tffeat` to use TF feature_extraction .so library, 'pyfeat' to python_speech_feature
      feature_name: fbank # fbank, spec
      save_feat_path: null  # null for dump feat with same dir of wavs
      # fbank
      save_fbank: true # save fbank or power spec
      feature_size: 26 # extract feature size
      add_delta_deltas: true # delta deltas
      # log pwoer
      log_powspec: false # true, save log power spec; otherwise save power spec
      # cmvn
      cmvn: true # apply cmvn or generate cmvn
      cmvn_path: 'data/dump/cmvn.npy' # cmvn file
    text:
      enable: False
      vocab_path:  # path to vocab(default: 'vocab
      vocab_size: 5004 # vocab size
      max_text_len: 100 # max length for text
    classes:
      num: 4
      vocab:
        ang: 0
        hap: 1
        neu: 2
        sad: 3
    num_parallel_calls: 12
    num_prefetch_batch: 2
    shuffle_buffer_size: 200000
    need_shuffle: true

model:
  name: ResNet50 
  type: keras # raw, keras or eager model
  net:
    structure: null


solver:
  name: EmoKerasSolver
  quantization:
    enable: false # whether to quantization model
    quant_delay: 0 # Number of steps after which weights and activations are quantized during training
  adversarial:
    enable: false # whether to using adversiral training
    adv_alpha: 0.5 # adviseral alpha of loss
    adv_epslion: 0.1 # adviseral example epslion
  model_average:
    enable: false # use average model
    var_avg_decay: 0.99 # the decay rate of varaibles
  optimizer:
    name: adam
    epochs: 150 # aximum epochs
    batch_size: 64 # number of elements in a training batch
    loss: CrossEntropyLoss
    label_smoothing: 0.0 # label smoothing rate
    learning_rate:
      rate: 0.001 # learning rate of Adam optimizer
      min_rate: 0.00001
      patience: 10
      type: const # learning rate type: exp_decay, val_metric, custom
      decay_rate: 0.5  # the lr decay rate
      decay_steps: 1000  # the lr decay_step for optimizer
    clip_global_norm: 3.0 # clip global norm
    multitask: False # whether is multi-task
    early_stopping:
      enable: false 
  metrics:
    monitor_used: 'val_acc'
    metrics_used : ['accuracy']
    pos_label: 1 # int, same to sklearn
    cals:
    - name: AccuracyCal
      arguments: null 
    - name: ConfusionMatrixCal
      arguments: null
    - name: PrecisionCal
      arguments:
        average: 'weighted'
    - name: RecallCal
      arguments:
        average: 'weighted'
    - name: F1ScoreCal
      arguments:
        average: 'weighted'
  postproc:
      name: EmoPostProc
      log_verbose: false 
      eval: true # compute metrics
      infer: true  # get predict results
      pred_path: null # None for `model_path`/infer, dumps infer output to this dir
      thresholds:
          - 0.5
      smoothing:
          enable: true
          count: 2
  saver:
    model_path: "exp/emo/keras-reset50/7s-ls0.0-decay1.0-batch64-all"
    max_to_keep: 10
    save_checkpoints_steps: 100
    keep_checkpoint_every_n_hours: 10000
    checkpoint_every: 100 # the step to save checkpoint
    summary: false
    save_summary_steps: 100
    eval_on_dev_every_secs: 1
    print_every: 10
    resume_model_path: ""
  loader:
    model_load_type: null #restore which kind of model(support 4 values: "best", "lastest", "scratch", "specific")
    init_epoch: 0 #epoch at which to start training(range from 0 to solver.optimizer.epochs)
    file_name: null
  run_config:
    debug: false # use tfdbug
    tf_random_seed: null # 0-2**32; null is None, try to read data from /dev/urandom if available or seed from the clock otherwise
    allow_soft_placement: true
    log_device_placement: false
    intra_op_parallelism_threads: 10
    inter_op_parallelism_threads: 10
    allow_growth: true
    log_step_count_steps: 100 #The frequency, in number of global steps, that the global step/sec and the loss will be logged during training.
  run_options:
    trace_level: 3 # 0: no trace, 1: sotware trace, 2: hardware_trace, 3: full trace
    inter_op_thread_pool: -1
    report_tensor_allocations_upon_oom: true
  distilling:
    enable: false 
    name : Teacher
    loss : DistillationLoss
    temperature: 5
    alpha: 0.5
    teacher_model: /model/path/frozen_graph.pb


serving:
  enable: true
  name : Evaluate
  model: /model/path/frozen_graph.pb # saved model dir, ckpt dir, or frozen_model.pb
  inputs: 'inputs:0'
  outpus: 'softmax_output:0'

