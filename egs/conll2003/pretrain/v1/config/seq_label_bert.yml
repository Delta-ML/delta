---
data:
  train:
    paths:
      - "egs/conll2003/pretrain/v1/data/train.txt.bert.new"
  eval:
    paths:
      - "egs/conll2003/pretrain/v1/data/dev.txt.bert.new"
  infer:
    paths:
      - "egs/conll2003/pretrain/v1/data/test.txt.bert.new"
  task:
    name: TextSeqLabelTask
    preparer:
      enable: true
      name: TextSeqLabelPreparer
      done_sign: "egs/conll2003/pretrain/v1/exp_bert/prepare.done"
      reuse: true
    use_dense: false
    language: english
    split_by_space: true
    vocab_min_frequency: 0
    use_custom_vocab: True
    text_vocab: "egs/conll2003/pretrain/v1/exp_bert/text_bert_vocab.txt"
    label_vocab: "egs/conll2003/pretrain/v1/exp_bert/label_bert_vocab.txt"
    max_seq_len: 200
    num_classes: 10
    batch_size: 16
    epochs: 20
    num_parallel_calls: 12
    num_prefetch_batch: 2
    shuffle_buffer_size: 14041
    need_shuffle: true
    classes:
      positive_id: 1
      num_classes: 9
      vocab:
        O: 0
        B-PER: 1
        I-PER: 2
        B-LOC: 3
        I-LOC: 4
        B-ORG: 5
        I-ORG: 6
        B-MISC: 7
        I-MISC: 8

model:
  name: BilstmCrfModel
  type: keras # raw, keras or eager model
  use_pre_train_emb: false
  pre_train_emb_path: "egs/conll2003/pretrain/v1/data/glove.840B.300d.txt"
  embedding_path: "egs/conll2003/pretrain/v1/data/embeding.pkl"
  use_pre_train_model: true
  pre_train_model:
    name: 'bert'
    mode: 'feature'
    output: 'seq' #seq/cls
    dim: 1024
    path: "egs/conll2003/pretrain/v1/data/bert_model_placeholder.ckpt"
    seg: 102
    cls: 101
    pad: 0
    layers: 23
  net:
    structure:
      embedding_size: 300
      cell_type: lstm
      cell_dim: 100
      num_units: 300
      num_layers: 1
      batch_size: 16
      max_len: 200
      dropout_rate: 0.5
      l2_reg_lambda: 0
      fc_dim: 100

solver:
  name: PretrainRawSeqLabelSolver
  quantization:
    enable: false # whether to quantization model
    quant_delay: 0 # Number of steps after which weights and activations are quantized during training
  adversarial:
    enable: false # whether to using adversiral training
    adv_alpha: 0.5 # adviseral alpha of loss
    adv_epslion: 0.1 # adviseral example epslion
  model_average:
    enable: false # use average model
    var_avg_decay: 0.99 # the decay rate of varaibles
  optimizer:
    name: adam
    loss: CrfLoss
    label_smoothing: 0.0 # label smoothing rate
    learning_rate:
      rate: 0.001 # learning rate of Adam optimizer
      type:  const # learning rate type
      num_warmup_steps: 10000
      decay_rate: 0.99  # the lr decay rate
      decay_steps: 100  # the lr decay_step for optimizer
    batch_size: 16
    epochs: 3
    clip_global_norm: 5.0 # clip global norm
    multitask: False # whether is multi-task
  metrics:
    pos_label: 1
    cals:
      - name: CrfCal
        arguments:
          label_vocab_path: "egs/conll2003/pretrain/v1/exp_bert/label_bert_vocab.txt"
  postproc:
    name: SavePredEntityPostProc
    res_file: "egs/conll2003/pretrain/v1/res/infer_res.txt"
  saver:
    model_path: "egs/conll2003/pretrain/v1/bert_ckpt/bilstmcrf_bert/"
    max_to_keep: 1
    save_checkpoint_steps: 100
    print_every: 10
  service:
    model_path: "egs/conll2003/pretrain/v1/exp/bilstmcrf_bert/service"
    model_version: "1"
  run_config:
    tf_random_seed: null
    allow_soft_placement: true
    log_device_placement: false
    intra_op_parallelism_threads: 10
    inter_op_parallelism_threads: 10
    allow_growth: true
